{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 作業Day49~Day50_Final Topic\n",
    "\n",
    "該版本的 yolov3 實現邏輯主要寫在 yolo.py 中 YOLO 這個 class 的 detect_image ，其回傳已畫上檢測到的 bboxes 和物件類別的圖片。\n",
    "請嘗試閱讀及盡量理解 detect_image 的程式碼片段\n",
    "請修改/模仿 detect_image 的寫法，使其回傳 bboxes 的信息、信心度及 bboxes 對應的類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x # 確保 colob 中使用的 tensorflow 是 1.x 版本而不是 tensorflow 2\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive') # 將 google drive 掛載在 colob，\n",
    "# 下載基於 keras 的 yolov3 程式碼\n",
    "%cd 'gdrive/My Drive'\n",
    "# !git clone https://github.com/qqwweee/keras-yolo3 # 如果之前已經下載過就可以註解掉\n",
    "%cd keras-yolo3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "if not os.path.exists(\"model_data/trained_weights_final.h5\"):\n",
    "  # 下載 yolov3 的網路權重，並且把權重轉換為 keras 能夠讀取的格式\n",
    "    print(\"Model doesn't exist, downloading...\")\n",
    "   # os.system(\"wget https://pjreddie.com/media/files/yolov3.weights\")\n",
    "    print(\"Converting yolov3.weights to yolo.h5...\")\n",
    "    os.system(\"python convert.py yolov3.cfg yolov3.weights model_data/trained_weights_final.h5\")\n",
    "else:\n",
    "    print(\"Model exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## STEP 1. 載入相關函式庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities相關函式庫\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 多維向量處理相關函式庫\n",
    "import numpy as np\n",
    "\n",
    "# 讓Keras只使用GPU來進行訓練\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# 圖像處理相關函式庫\n",
    "import cv2\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import colorsys\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "%matplotlib inline\n",
    "\n",
    "# 序列/反序列化相關函式庫\n",
    "import pickle\n",
    "\n",
    "# 深度學習相關函式庫\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.merge import concatenate\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# 專案相關函式庫\n",
    "from preprocessing import parse_annotation, BatchGenerator\n",
    "from utils import WeightReader, decode_netout, draw_boxes, normalize\n",
    "from utils import draw_bgr_image_boxes, draw_rgb_image_boxes, draw_pil_image_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2. 設定參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 專案的根目錄路徑\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# 訓練/驗證用的資料目錄\n",
    "DATA_PATH = os.path.join(ROOT_DIR, \"data\")\n",
    "\n",
    "# 資料集目錄\n",
    "DATA_SET_PATH = os.path.join(DATA_PATH, \"hands\")\n",
    "TRAIN_DATA_PATH = os.path.join(DATA_SET_PATH, \"train\")\n",
    "\n",
    "TRAIN_IMGS_PATH = os.path.join(TRAIN_DATA_PATH, \"pos\")\n",
    "TRAIN_ANNOTATION_PATH = os.path.join(TRAIN_DATA_PATH, \"posGt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義圖像的類別\n",
    "在這個圖像資料集裡有2種類別:\n",
    "袋鼠 \n",
    "熊\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 圖像類別的Label-encoding\n",
    "map_categories = {0: 'kangaroo', 1: 'raccoon'}\n",
    "\n",
    "# 取得所有圖像的圖像類別列表\n",
    "labels=list(map_categories.values())\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定YOLOv2模型的設定與參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = labels # 圖像類別\n",
    "\n",
    "IMAGE_H, IMAGE_W = 416, 416 # 模型輸入的圖像長寬\n",
    "GRID_H,  GRID_W  = 13 , 13\n",
    "BOX              = 5\n",
    "CLASS            = len(LABELS)\n",
    "CLASS_WEIGHTS    = np.ones(CLASS, dtype='float32')\n",
    "OBJ_THRESHOLD    = 0.5\n",
    "NMS_THRESHOLD    = 0.45 # NMS非極大值抑制 , 說明(https://chenzomi12.github.io/2016/12/14/YOLO-nms/)\n",
    "ANCHORS          = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n",
    "\n",
    "NO_OBJECT_SCALE  = 1.0\n",
    "OBJECT_SCALE     = 5.0\n",
    "COORD_SCALE      = 1.0\n",
    "CLASS_SCALE      = 1.0\n",
    "\n",
    "BATCH_SIZE       = 16\n",
    "WARM_UP_BATCHES  = 0\n",
    "TRUE_BOX_BUFFER  = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Darknet預訓練權重檔與訓練/驗證資料目錄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_path = 'yolo.weights' \n",
    "\n",
    "train_image_folder = TRAIN_IMGS_PATH\n",
    "train_annot_folder = TRAIN_ANNOTATION_PATH\n",
    "valid_image_folder = TRAIN_IMGS_PATH\n",
    "valid_annot_folder = TRAIN_ANNOTATION_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2. 構建YOLOv2網絡結構模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to implement the orgnization layer (thanks to github.com/allanzelener/YAD2K)\n",
    "def space_to_depth_x2(x):\n",
    "    return tf.space_to_depth(x, block_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_image = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
    "true_boxes  = Input(shape=(1, 1, 1, TRUE_BOX_BUFFER , 4))\n",
    "\n",
    "# Layer 1\n",
    "x = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_1', use_bias=False)(input_image)\n",
    "x = BatchNormalization(name='norm_1')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 2\n",
    "x = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_2')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 3\n",
    "x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_3', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_3')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 4\n",
    "x = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_4', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_4')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 5\n",
    "x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_5', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_5')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 6\n",
    "x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_6')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 7\n",
    "x = Conv2D(128, (1,1), strides=(1,1), padding='same', name='conv_7', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_7')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 8\n",
    "x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_8')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 9\n",
    "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_9', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_9')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 10\n",
    "x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_10', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_10')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 11\n",
    "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_11', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_11')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 12\n",
    "x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_12', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_12')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 13\n",
    "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_13', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_13')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "skip_connection = x\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 14\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_14', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_14')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 15\n",
    "x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_15', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_15')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 16\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_16', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_16')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 17\n",
    "x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_17', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_17')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 18\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_18', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_18')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 19\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_19', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_19')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 20\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_20', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_20')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 21\n",
    "skip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_connection)\n",
    "skip_connection = BatchNormalization(name='norm_21')(skip_connection)\n",
    "skip_connection = LeakyReLU(alpha=0.1)(skip_connection)\n",
    "skip_connection = Lambda(space_to_depth_x2)(skip_connection)\n",
    "\n",
    "x = concatenate([skip_connection, x])\n",
    "\n",
    "# Layer 22\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_22', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_22')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 23\n",
    "x = Conv2D(BOX * (4 + 1 + CLASS), (1,1), strides=(1,1), padding='same', name='conv_23')(x)\n",
    "output = Reshape((GRID_H, GRID_W, BOX, 4 + 1 + CLASS))(x)\n",
    "\n",
    "# small hack to allow true_boxes to be registered when Keras build the model \n",
    "# for more information: https://github.com/fchollet/keras/issues/2790\n",
    "output = Lambda(lambda args: args[0])([output, true_boxes])\n",
    "\n",
    "model = Model([input_image, true_boxes], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() # 打印模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3. 載入預訓練的模型權重¶\n",
    "Load the weights originally provided by YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_reader = WeightReader(wt_path)  # 初始讀取Darknet預訓練權重檔物件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_reader.reset()\n",
    "nb_conv = 23 # 總共有23層的卷積層\n",
    "\n",
    "for i in range(1, nb_conv+1):\n",
    "    conv_layer = model.get_layer('conv_' + str(i))\n",
    "    \n",
    "    # 在conv_1~conv_22的卷積組合裡都包含了\"conv + norm\"二層, 只有conv_23是獨立一層    \n",
    "    if i < nb_conv: \n",
    "        print(\"handle norm_\" + str(i) + \" start\")        \n",
    "        norm_layer = model.get_layer('norm_' + str(i)) # 取得BatchNormalization層\n",
    "        \n",
    "        size = np.prod(norm_layer.get_weights()[0].shape) # 取得BatchNormalization層的參數量\n",
    "        print(\"shape: \", norm_layer.get_weights()[0].shape)\n",
    "        \n",
    "        beta  = weight_reader.read_bytes(size)\n",
    "        gamma = weight_reader.read_bytes(size)\n",
    "        mean  = weight_reader.read_bytes(size)\n",
    "        var   = weight_reader.read_bytes(size)\n",
    "        weights = norm_layer.set_weights([gamma, beta, mean, var])\n",
    "        print(\"handle norm_\" + str(i) + \" completed\")\n",
    "        \n",
    "    if len(conv_layer.get_weights()) > 1:\n",
    "        print(\"handle conv_\" + str(i) + \" start\")  \n",
    "        print(\"len:\",len(conv_layer.get_weights()))\n",
    "        bias   = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n",
    "        kernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "        kernel = kernel.transpose([2,3,1,0])\n",
    "        conv_layer.set_weights([kernel, bias])\n",
    "        print(\"handle conv_\" + str(i) + \" completed\")\n",
    "    else:\n",
    "        print(\"handle conv_\" + str(i) + \" start\")        \n",
    "        kernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "        kernel = kernel.transpose([2,3,1,0])\n",
    "        conv_layer.set_weights([kernel])\n",
    "        print(\"handle conv_\" + str(i) + \" completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4. 設定要微調(fine-tune)的模型層級權重¶\n",
    "Randomize weights of the last layer\n",
    "\n",
    "由於在YOLOv2的模型中, 最後一層卷積層決定了最後的輸出, 讓我們重新來調整與訓練這一層的卷積層來讓預訓練的模型可以讓我們進行所需要的微調。 詳細的概念與說明,見 1.5: 使用預先訓練的卷積網絡模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer   = model.layers[-4] # 找出最後一層的卷積層\n",
    "weights = layer.get_weights()\n",
    "\n",
    "new_kernel = np.random.normal(size=weights[0].shape)/(GRID_H*GRID_W)\n",
    "new_bias   = np.random.normal(size=weights[1].shape)/(GRID_H*GRID_W)\n",
    "\n",
    "layer.set_weights([new_kernel, new_bias]) # 重初始化權重\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##STEP 5. 模型訓練\n",
    "YOLOv2訓練用的損失函數:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    mask_shape = tf.shape(y_true)[:4]\n",
    "    \n",
    "    cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)))\n",
    "    cell_y = tf.transpose(cell_x, (0,2,1,3,4))\n",
    "\n",
    "    cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [BATCH_SIZE, 1, 1, 5, 1])\n",
    "    \n",
    "    coord_mask = tf.zeros(mask_shape)\n",
    "    conf_mask  = tf.zeros(mask_shape)\n",
    "    class_mask = tf.zeros(mask_shape)\n",
    "    \n",
    "    seen = tf.Variable(0.)\n",
    "    total_recall = tf.Variable(0.)\n",
    "    \n",
    "    \"\"\"\n",
    "    Adjust prediction\n",
    "    \"\"\"\n",
    "    ### adjust x and y      \n",
    "    pred_box_xy = tf.sigmoid(y_pred[..., :2]) + cell_grid\n",
    "    \n",
    "    ### adjust w and h\n",
    "    pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(ANCHORS, [1,1,1,BOX,2])\n",
    "    \n",
    "    ### adjust confidence\n",
    "    pred_box_conf = tf.sigmoid(y_pred[..., 4])\n",
    "    \n",
    "    ### adjust class probabilities\n",
    "    pred_box_class = y_pred[..., 5:]\n",
    "    \n",
    "    \"\"\"\n",
    "    Adjust ground truth\n",
    "    \"\"\"\n",
    "    ### adjust x and y\n",
    "    true_box_xy = y_true[..., 0:2] # relative position to the containing cell\n",
    "    \n",
    "    ### adjust w and h\n",
    "    true_box_wh = y_true[..., 2:4] # number of cells accross, horizontally and vertically\n",
    "    \n",
    "    ### adjust confidence\n",
    "    true_wh_half = true_box_wh / 2.\n",
    "    true_mins    = true_box_xy - true_wh_half\n",
    "    true_maxes   = true_box_xy + true_wh_half\n",
    "    \n",
    "    pred_wh_half = pred_box_wh / 2.\n",
    "    pred_mins    = pred_box_xy - pred_wh_half\n",
    "    pred_maxes   = pred_box_xy + pred_wh_half       \n",
    "    \n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    \n",
    "    true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
    "    pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "    \n",
    "    true_box_conf = iou_scores * y_true[..., 4]\n",
    "    \n",
    "    ### adjust class probabilities\n",
    "    true_box_class = tf.argmax(y_true[..., 5:], -1)\n",
    "    \n",
    "    \"\"\"\n",
    "    Determine the masks\n",
    "    \"\"\"\n",
    "    ### coordinate mask: simply the position of the ground truth boxes (the predictors)\n",
    "    coord_mask = tf.expand_dims(y_true[..., 4], axis=-1) * COORD_SCALE\n",
    "    \n",
    "    ### confidence mask: penelize predictors + penalize boxes with low IOU\n",
    "    # penalize the confidence of the boxes, which have IOU with some ground truth box < 0.6\n",
    "    true_xy = true_boxes[..., 0:2]\n",
    "    true_wh = true_boxes[..., 2:4]\n",
    "    \n",
    "    true_wh_half = true_wh / 2.\n",
    "    true_mins    = true_xy - true_wh_half\n",
    "    true_maxes   = true_xy + true_wh_half\n",
    "    \n",
    "    pred_xy = tf.expand_dims(pred_box_xy, 4)\n",
    "    pred_wh = tf.expand_dims(pred_box_wh, 4)\n",
    "    \n",
    "    pred_wh_half = pred_wh / 2.\n",
    "    pred_mins    = pred_xy - pred_wh_half\n",
    "    pred_maxes   = pred_xy + pred_wh_half    \n",
    "    \n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    \n",
    "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "\n",
    "    best_ious = tf.reduce_max(iou_scores, axis=4)\n",
    "    conf_mask = conf_mask + tf.to_float(best_ious < 0.6) * (1 - y_true[..., 4]) * NO_OBJECT_SCALE\n",
    "    \n",
    "    # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\n",
    "    conf_mask = conf_mask + y_true[..., 4] * OBJECT_SCALE\n",
    "    \n",
    "    ### class mask: simply the position of the ground truth boxes (the predictors)\n",
    "    class_mask = y_true[..., 4] * tf.gather(CLASS_WEIGHTS, true_box_class) * CLASS_SCALE       \n",
    "    \n",
    "    \"\"\"\n",
    "    Warm-up training\n",
    "    \"\"\"\n",
    "    no_boxes_mask = tf.to_float(coord_mask < COORD_SCALE/2.)\n",
    "    seen = tf.assign_add(seen, 1.)\n",
    "    \n",
    "    true_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(seen, WARM_UP_BATCHES), \n",
    "                          lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask, \n",
    "                                   true_box_wh + tf.ones_like(true_box_wh) * np.reshape(ANCHORS, [1,1,1,BOX,2]) * no_boxes_mask, \n",
    "                                   tf.ones_like(coord_mask)],\n",
    "                          lambda: [true_box_xy, \n",
    "                                   true_box_wh,\n",
    "                                   coord_mask])\n",
    "    \n",
    "    \"\"\"\n",
    "    Finalize the loss\n",
    "    \"\"\"\n",
    "    nb_coord_box = tf.reduce_sum(tf.to_float(coord_mask > 0.0))\n",
    "    nb_conf_box  = tf.reduce_sum(tf.to_float(conf_mask  > 0.0))\n",
    "    nb_class_box = tf.reduce_sum(tf.to_float(class_mask > 0.0))\n",
    "    \n",
    "    loss_xy    = tf.reduce_sum(tf.square(true_box_xy-pred_box_xy)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_wh    = tf.reduce_sum(tf.square(true_box_wh-pred_box_wh)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_conf  = tf.reduce_sum(tf.square(true_box_conf-pred_box_conf) * conf_mask)  / (nb_conf_box  + 1e-6) / 2.\n",
    "    loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
    "    loss_class = tf.reduce_sum(loss_class * class_mask) / (nb_class_box + 1e-6)\n",
    "    \n",
    "    loss = loss_xy + loss_wh + loss_conf + loss_class\n",
    "    \n",
    "    nb_true_box = tf.reduce_sum(y_true[..., 4])\n",
    "    nb_pred_box = tf.reduce_sum(tf.to_float(true_box_conf > 0.5) * tf.to_float(pred_box_conf > 0.3))\n",
    "\n",
    "    \"\"\"\n",
    "    Debugging code\n",
    "    \"\"\"    \n",
    "    current_recall = nb_pred_box/(nb_true_box + 1e-6)\n",
    "    total_recall = tf.assign_add(total_recall, current_recall) \n",
    "\n",
    "    loss = tf.Print(loss, [tf.zeros((1))], message='Dummy Line \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_class], message='Loss Class \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [current_recall], message='Current Recall \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [total_recall/seen], message='Average Recall \\t', summarize=1000)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用來產生Keras訓練模型的BatchGenerator的設定:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_config = {\n",
    "    'IMAGE_H'         : IMAGE_H, # YOLOv2網絡輸入的image_h\n",
    "    'IMAGE_W'         : IMAGE_W, # YOLOv2網絡輸入的image_w\n",
    "    'GRID_H'          : GRID_H,  # 直向網格的拆分數量\n",
    "    'GRID_W'          : GRID_W,  # 橫向網格的拆分數量\n",
    "    'BOX'             : BOX,     # 每個單一網格要預測的邊界框數量\n",
    "    'LABELS'          : LABELS,  # 要預測的圖像種類列表\n",
    "    'CLASS'           : len(LABELS), # 要預測的圖像種類數\n",
    "    'ANCHORS'         : ANCHORS, # 每個單一網格要預測的邊界框時用的錨點\n",
    "    'BATCH_SIZE'      : BATCH_SIZE, # 訓練時的批量數\n",
    "    'TRUE_BOX_BUFFER' : 50, # 一個訓練圖像最大數量的邊界框數\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析圖像標註檔\n",
    "由於這個資料集的標註檔並不是採用PASCAL VOC格式而是自行定義的格式\n",
    "因此要對標註檔的解析進行客製化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "def parse_hands_annotation(ann_dir, img_dir, labels=[]):\n",
    "    \"\"\"解析圖像標註檔\n",
    "\n",
    "    根據手部標註檔存放的目錄路徑迭代地解析每一個標註檔，\n",
    "    將每個圖像的檔名(filename)、圖像的寬(width)、高(height)、圖像的類別(name)以\n",
    "    及物體的邊界框的坐標(xmin,ymin,xmax,ymax)擷取出來。以下是圖像標註檔的範例:\n",
    "    \n",
    "    % bbGt version=3\n",
    "    leftHand_driver 87 295 57 67 0 0 0 0 0 0 0\n",
    "    rightHand_driver 223 283 62 64 0 0 0 0 0 0 0\n",
    "    leftHand_passenger 483 356 91 71 0 0 0 0 0 0 0\n",
    "    rightHand_passenger 548 328 86 70 0 0 0 0 0 0 0\n",
    "    \n",
    "    擷取目標: [hands_class x y w h ...]\n",
    "    hands_class: leftHand_driver/leftHand_passenger: left_hand, rightHand_driver/rightHand_passenger: right_hand, \n",
    "\n",
    "    參數:\n",
    "        ann_dir: 圖像標註檔存放的目錄路徑\n",
    "        img_dir: 圖像檔存放的目錄路徑\n",
    "        labels: 圖像資料集的物體類別列表\n",
    "\n",
    "    回傳:\n",
    "        all_imgs: 一個列表物件, 每一個物件都包括了要訓練用的重要資訊。例如:\n",
    "                    {\n",
    "                        'filename': '/tmp/img/img001.jpg',\n",
    "                        'width': 128,\n",
    "                        'height': 128,\n",
    "                        'object':[\n",
    "                            {'name':'person',xmin:0, ymin:0, xmax:28, ymax:28},\n",
    "                            {'name':'person',xmin:45, ymin:45, xmax:60, ymax:60}\n",
    "                        ]\n",
    "                    }\n",
    "        seen_labels: 一個字典物件(k:圖像類別, v:出現的次數)用來檢視每一個圖像類別出現的次數\n",
    "    \"\"\"\n",
    "    print(\"start parsing annotation..\")\n",
    "    \n",
    "    # 產生一個標註圖資料標註的mapping\n",
    "    hands_label_map = {'leftHand_driver':'left_hand', 'leftHand_passenger':'left_hand',\n",
    "                      'rightHand_driver':'right_hand', 'rightHand_passenger':'right_hand'}\n",
    "    all_imgs = []\n",
    "    seen_labels = {}\n",
    "    \n",
    "    # 迭代每個標註檔\n",
    "    for ann in tqdm(sorted(os.listdir(ann_dir))):\n",
    "        img = {'object':[]}\n",
    "        # 處理圖檔檔案路徑\n",
    "        img_filename = ann[0:len(ann)-3]+\"png\"\n",
    "                \n",
    "        # 圖檔檔案路徑\n",
    "        img['filename'] = os.path.join(img_dir, img_filename)\n",
    "        \n",
    "        im = Image.open(img['filename'])\n",
    "        img_width, img_height = im.size\n",
    "        \n",
    "        # 圖檔大小\n",
    "        img['width'] = img_width\n",
    "        img['height'] = img_height\n",
    "        \n",
    "        line = 0 # 行數\n",
    "        with open(os.path.join(ann_dir, ann), 'r') as fann:\n",
    "            # 一行一行讀進來處理\n",
    "            for cnt, line in enumerate(fann):\n",
    "                # 忽略第一行的資料\n",
    "                if cnt == 0:\n",
    "                    continue\n",
    "                # 建立物件來保留bbox\n",
    "                obj = {}\n",
    "                \n",
    "                tokens = line.split()\n",
    "                label = hands_label_map[tokens[0]]\n",
    "                bbox_x = int(tokens[1])\n",
    "                bbox_y = int(tokens[2])\n",
    "                bbox_w = int(tokens[3])\n",
    "                bbox_h = int(tokens[4])\n",
    "                #print(\"Line {}: {},{},{},{},{}\".format(cnt, label, bbox_x, bbox_y, bbox_w, bbox_h))\n",
    "                \n",
    "                obj['name'] = label\n",
    "                \n",
    "                if obj['name'] in seen_labels:\n",
    "                    seen_labels[obj['name']] += 1\n",
    "                else:\n",
    "                    seen_labels[obj['name']] = 1\n",
    "                \n",
    "                obj['xmin'] = bbox_x\n",
    "                obj['ymin'] = bbox_y\n",
    "                obj['xmax'] = bbox_x + bbox_w\n",
    "                obj['ymax'] = bbox_y + bbox_h\n",
    "                \n",
    "                #檢看是是否有物體的標籤是沒有在傳入的物體類別(labels)中\n",
    "                if len(labels) > 0 and obj['name'] not in labels:\n",
    "                    continue\n",
    "                else:\n",
    "                    img['object'] += [obj]\n",
    "            \n",
    "            # 把img物件加進要回傳的列表中\n",
    "            if len(['object']) > 0:\n",
    "                all_imgs += [img]\n",
    "    \n",
    "    print(\"Parsing annotation completed!\")\n",
    "    print(\"Total: {} images processed.\".format(len(all_imgs)))\n",
    "    return all_imgs, seen_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 進行圖像標註檔的解析 (在Racoon資料集的標註採用的是PASCAL VOC的XML格式)\n",
    "train_imgs, seen_train_labels = parse_hands_annotation(train_annot_folder, train_image_folder, labels=LABELS)\n",
    "\n",
    "# 建立一個訓練用的資料產生器\n",
    "train_batch = BatchGenerator(train_imgs, generator_config, norm=normalize)\n",
    "\n",
    "# 進行圖像標註檔的解析 (在Racoon資料集的標註採用的是PASCAL VOC的XML格式)\n",
    "valid_imgs, seen_valid_labels = parse_hands_annotation(valid_annot_folder, valid_image_folder, labels=LABELS)\n",
    "\n",
    "# 建立一個驗證用的資料產生器\n",
    "valid_batch = BatchGenerator(valid_imgs, generator_config, norm=normalize, jitter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設置一些回調函式並開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果超過3次的循環在loss的收歛上沒有改善就停止訓練 \n",
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           min_delta=0.001, \n",
    "                           patience=3, \n",
    "                           mode='min', \n",
    "                           verbose=1)\n",
    "\n",
    "# 每次的訓練循都去比較模型的loss是否有改善, 有就把模型的權重儲存下來\n",
    "checkpoint = ModelCheckpoint('weights_hands.h5', \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min', \n",
    "                             period=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "#optimizer = SGD(lr=1e-4, decay=0.0005, momentum=0.9)\n",
    "#optimizer = RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss=custom_loss, optimizer=optimizer)\n",
    "\n",
    "history = model.fit_generator(generator = train_batch, \n",
    "                    steps_per_epoch  = len(train_batch), \n",
    "                    epochs           = 20, # 應該增加更多次的訓練循環 \n",
    "                    verbose          = 0,\n",
    "                    validation_data  = valid_batch,\n",
    "                    validation_steps = len(valid_batch),\n",
    "                    callbacks        = [early_stop, checkpoint], \n",
    "                    max_queue_size   = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6. 圖像的物體偵測\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入訓練好的模型權重\n",
    "model.load_weights(\"weights_hands.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 產生一個Dummy的標籤輸入\n",
    "\n",
    "# 在訓練階段放的是真實的邊界框與圖像類別訊息\n",
    "# 但在預測階段還是需要有一個Dummy的輸入, 因為定義在網絡的結構中有兩個輸入： \n",
    "#   1.圖像的輸人 \n",
    "#   2.圖像邊界框/錨點/信心分數的輸入\n",
    "dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 選一張圖像\n",
    "img_filepath = train_imgs[np.random.randint(len(train_imgs))]['filename']\n",
    "\n",
    "# 使用OpenCV讀入圖像\n",
    "image = cv2.imread(img_filepath) # 載入圖像\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# 進行圖像輸入的前處理\n",
    "input_image = cv2.resize(image, (416, 416)) # 修改輸入圖像大小來符合模型的要求\n",
    "input_image = input_image / 255. # 進行圖像歸一處理\n",
    "input_image = np.expand_dims(input_image, 0) # 增加 batch dimension\n",
    "\n",
    "# 進行圖像偵測\n",
    "netout = model.predict([input_image, dummy_array])\n",
    "\n",
    "# 解析網絡的輸出來取得最後偵測出來的邊界框(bounding boxes)列表\n",
    "boxes = decode_netout(netout[0], \n",
    "                      obj_threshold=OBJ_THRESHOLD,\n",
    "                      nms_threshold=NMS_THRESHOLD,\n",
    "                      anchors=ANCHORS, \n",
    "                      nb_class=CLASS)\n",
    "\n",
    "# \"draw_bgr_image_boxes\"\n",
    "# 一個簡單把邊界框與預測結果打印到原始圖像(BGR)上的工具函式\n",
    "# 參數: image 是image的numpy ndarray [h, w, channels(BGR)]\n",
    "#       boxes 是偵測的結果\n",
    "#       labels 是模型訓練的圖像類別列表\n",
    "# 回傳： image 是image的numpy ndarray [h, w, channels(RGB)]\n",
    "image = draw_bgr_image_boxes(image, boxes, labels=LABELS)\n",
    "\n",
    "# 把最後的結果秀出來\n",
    "plt.imshow(image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6. 影像的物體偵測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入訓練好的模型權重\n",
    "model.load_weights(\"weights_hands.h5\")\n",
    "\n",
    "# 產生一個Dummy的標籤輸入\n",
    "\n",
    "# 在訓練階段放的是真實的邊界框與圖像類別訊息\n",
    "# 但在預測階段還是需要有一個Dummy的輸入, 因為定義在網絡的結構中有兩個輸入： \n",
    "#   1.圖像的輸人 \n",
    "#   2.圖像邊界框/錨點/信心分數的輸入\n",
    "dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料集目錄\n",
    "VIDEO_DATA_PATH = os.path.join(DATA_SET_PATH, \"video\")\n",
    "\n",
    "# 選擇要進行浣熊影像偵測的影像檔\n",
    "# 在這個測試我從YOUTUBE下載了: https://www.youtube.com/watch?v=c0IykwK6zkY\n",
    "video_inp =  os.path.join(VIDEO_DATA_PATH, \"cardriving.mp4\")\n",
    "\n",
    "# 偵測結果的輸出影像檔\n",
    "video_out =  os.path.join(VIDEO_DATA_PATH, \"cardriving-out.mp4\")\n",
    "\n",
    "# 透過OpenCv擷取影像\n",
    "video_reader = cv2.VideoCapture(video_inp)\n",
    "\n",
    "# 取得影像的基本資訊\n",
    "nb_frames = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT)) # 總共有多少frames\n",
    "frame_h = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))  # 每個frame的高\n",
    "frame_w = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))   # 每個frame的寬\n",
    "\n",
    "# 設定影像的輸出\n",
    "video_writer = cv2.VideoWriter(video_out,\n",
    "                               cv2.VideoWriter_fourcc(*'XVID'), \n",
    "                               50.0, \n",
    "                               (frame_w, frame_h))\n",
    "\n",
    "# 迭代每一個frame來進行圖像偵測\n",
    "for i in tqdm(range(nb_frames)):\n",
    "    ret, image = video_reader.read() # 讀取一個frame\n",
    "    \n",
    "    input_image = cv2.resize(image, (416, 416)) # 修改輸入圖像大小來符合模型的要求\n",
    "    input_image = input_image / 255. # 進行圖像歸一處理\n",
    "    input_image = np.expand_dims(input_image, 0) # 增加 batch dimension\n",
    "\n",
    "    # 進行圖像偵測\n",
    "    netout = model.predict([input_image, dummy_array])\n",
    "\n",
    "    # 解析網絡的輸出來取得最後偵測出來的邊界框(bounding boxes)列表\n",
    "    boxes = decode_netout(netout[0], \n",
    "                          obj_threshold=OBJ_THRESHOLD,\n",
    "                          nms_threshold=NMS_THRESHOLD,\n",
    "                          anchors=ANCHORS, \n",
    "                          nb_class=CLASS)\n",
    "    \n",
    "    # \"draw_bgr_image_boxes\"\n",
    "    # 一個簡單把邊界框與預測結果打印到原始圖像(BGR)上的工具函式\n",
    "    # 參數: image 是image的numpy ndarray [h, w, channels(BGR)]\n",
    "    #       boxes 是偵測的結果\n",
    "    #       labels 是模型訓練的圖像類別列表\n",
    "    # 回傳： image 是image的numpy ndarray [h, w, channels(RGB)]\n",
    "    image = draw_bgr_image_boxes(image, boxes, labels=LABELS)\n",
    "\n",
    "    # 透過OpenCV把影像輸出出來\n",
    "    video_writer.write(np.uint8(image[:,:,::-1])) # 轉換 RGB -> BGR來讓Open CV寫Video\n",
    "    \n",
    "video_reader.release() # 釋放資源\n",
    "video_writer.release() # 釋放資源"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
